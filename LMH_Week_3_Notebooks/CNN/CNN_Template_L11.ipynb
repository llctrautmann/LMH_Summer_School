{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models,transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10(batch_sz, path='./datasets'):\n",
    "    num_classes = 10\n",
    "    transform_train = transforms.Compose([\n",
    "                        transforms.RandomCrop(32, padding=4), # Increase the number of samples by moving the images around \n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                    ])\n",
    "    transform_test = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "    # Training dataset\n",
    "    train_data = CIFAR10(root=path, train=True, download=True, transform=transform_train) # \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_sz,\n",
    "                                               shuffle=True, pin_memory=True)\n",
    "\n",
    "    # Test dataset\n",
    "    test_data = CIFAR10(root=path, train=False, download=True, transform=transform_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_sz, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lecture11CNN(nn.Module):\n",
    "    # Initialise / Constructor\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,10,3,1,1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(10) # has to be the same as rh\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64,128,3,1,1)\n",
    "        self.conv3 = nn.Conv2d(128,265,3,1,1)\n",
    "        self.conv4 = nn.Conv2d(256,128,3,1,1)\n",
    "        self.conv5 = nn.Conv2d(128,64,3,1,1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d((2,2))\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "        self.fc6 = nn.Linear(64*4*4,100)\n",
    "        self.fc7 = nn.Linear(100,10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.leaky_relu(self.conv5(x))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = x.view(-1, 4*4*64)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.fc7(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "\n",
    "model_2 = CNN().to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "# Loss and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=lr, momentum= mm)\n",
    "\n",
    "writer = SummaryWriter(f'runs/MNIST/CNN_1_64_1e-2')\n",
    "\n",
    "total_loss = []\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epoch_no):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # forwards\n",
    "        logits = model_2(data)\n",
    "        loss = criterion(logits, targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "        # backward \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Running Accuracy\n",
    "\n",
    "        _, predictions = logits.max(1)\n",
    "        num_corr = (predictions == targets).sum()\n",
    "        running_acc = float(num_corr)/float(data.shape[0])\n",
    "        \n",
    "        writer.add_scalar(\"Training Loss\", loss, global_step = step) \n",
    "        writer.add_scalar(\"Accuracy\", running_acc, global_step=step)\n",
    "        step += 1  \n",
    "\n",
    "\n",
    "    total_loss.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saliency \n",
    "# which parts of the image activate the network the most \n",
    "\n",
    "X = batch[0].to(device)\n",
    "\n",
    "X = X[3,:,:,:]\n",
    "\n",
    "X.requires_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Summer_School",
   "language": "python",
   "name": "summer_school"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d410fd81c9a0b51d0e53167c40a880776428168a003798fed6e3e9082ee009aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

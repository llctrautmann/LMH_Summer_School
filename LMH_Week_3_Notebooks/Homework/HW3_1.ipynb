{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_mMw1jKdhNI"
      },
      "source": [
        "Perform MNIST classification using Neural Netwroks and Convolutional Neural Networks.\n",
        "\n",
        "1) Use 10 iterations for training\n",
        "\n",
        "\n",
        "2) Show the training loss for both networks on the same plot\n",
        "\n",
        "\n",
        "3) Compare the test loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models,transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "import torchmetrics as tm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MNIST\n",
        "def mnist(batch_sz, valid_size=0.2, shuffle=True, random_seed=2000):\n",
        "    num_classes = 10\n",
        "    transform_train = transforms.Compose([\n",
        "                        transforms.RandomCrop(28, padding=4),\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "    \n",
        "    \n",
        "    transform_test = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "    \n",
        "\n",
        "    # Training dataset\n",
        "    train_data = MNIST(root='./datasets', train=True, download=True, transform=transform_train)\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    if shuffle == True:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_sz ,pin_memory=True)\n",
        "\n",
        "    # Test dataset\n",
        "    test_data = MNIST(root='./datasets', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                              batch_size=batch_sz, shuffle=False, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x13ff43a60>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "batch_sz=64 # this is batch size i.e. the number of rows in a batch of data\n",
        "\n",
        "train_loader, test_loader=mnist(batch_sz) \n",
        "\n",
        "tl = iter(train_loader)\n",
        "batch = next(tl)\n",
        "\n",
        "\n",
        "plt.imshow(batch[0][6,:,:,:].squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FD6fwO1ddbEU"
      },
      "outputs": [],
      "source": [
        "# Define the classes for the classic Neural Network and the Convolutional Neural Network\n",
        "\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size=784, output_classes=10) -> None:\n",
        "        super(NN, self).__init__()\n",
        "        self._layer_in = nn.Linear(input_size, 1024)\n",
        "        self._layer_h = nn.Linear(1024,512)\n",
        "        self._layer_out = nn.Linear(512, output_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self._layer_in(x))\n",
        "        x = torch.relu(self._layer_h(x))\n",
        "        x = self._layer_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "class NNet(nn.Module):\n",
        "    def __init__(self, input_size=784, output_size=10, no_hidden_layers=5,hidden_layer_size=1024) -> None:\n",
        "        super(NNet,self).__init__()\n",
        "        self.deep_nn = nn.Sequential()\n",
        "        \n",
        "        for i in range(no_hidden_layers):\n",
        "            self.deep_nn.add_module(f'ff{i}', nn.Linear(input_size,hidden_layer_size))\n",
        "            self.deep_nn.add_module(f'activation{i}',nn.ReLU())\n",
        "            input_size = hidden_layer_size\n",
        "\n",
        "        self.deep_nn.add_module(f'classifier',nn.Linear(hidden_layer_size,output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        tensor = self.deep_nn(x)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "# CNN\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=1,num_classes=10) -> None:\n",
        "        super(CNN,self).__init__() # In the other tutorial he is using CNN and self within the super(CNN, self) brakets\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3),stride=(1,1),padding=(1,1))  #  3 -1 /2 ; 3 being the filter_size, with padding the image size stays the same\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3),stride=(1,1),padding=(1,1))\n",
        "        self._in = nn.Linear(16*7*7, 128)\n",
        "        # self.h1 = nn.Linear(1024,1024) # remove and adjust dims to recover AC2\n",
        "        # self.h2 = nn.Linear(1024,128)\n",
        "        self.lin_out = nn.Linear(128,num_classes) # change 128 to 16*7*7 for AC1\n",
        "\n",
        "\n",
        "    # Forward Pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Linear Model\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = torch.relu(self._in(x))\n",
        "        # x = torch.relu(self.h1(x))\n",
        "        # x = torch.relu(self.h2(x))\n",
        "        x = self.lin_out(x)\n",
        "            \n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "# Model 1 Dim Testing\n",
        "model_1 = NNet().to(device=device)\n",
        "x = torch.randn(64,784)\n",
        "print(model_1(x).shape)\n",
        "\n",
        "# Model 2 Dim Testing\n",
        "\n",
        "model_2 = CNN().to(device=device)\n",
        "x = torch.randn(64,1,28,28)\n",
        "print(model_2(x).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model CNN\n",
        "\n",
        "# model_2 = CNN().to(device=device)\n",
        "\n",
        "\n",
        "\n",
        "# # Training\n",
        "# # Loss and Optimizer\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model_2.parameters(), lr=lr, momentum= mm)\n",
        "\n",
        "# writer = SummaryWriter(f'runs/MNIST/CNN_1_64_1e-2')\n",
        "\n",
        "# total_loss = []\n",
        "\n",
        "# step = 0\n",
        "\n",
        "# mini_batches = [1,64,128,256,512,1024]\n",
        "\n",
        "# learning_rates = [0.5,1e-1,1e-2,1e-3,1e-4]\n",
        "\n",
        "\n",
        "# for epoch in range(epoch_no):\n",
        "#     train_loss = 0\n",
        "#     for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "#         data = data.to(device=device)\n",
        "#         targets = targets.to(device=device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         # forwards\n",
        "#         logits = model_2(data)\n",
        "#         loss = criterion(logits, targets)\n",
        "#         train_loss += loss.item()\n",
        "\n",
        "\n",
        "#         # backward \n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Running Accuracy\n",
        "\n",
        "#         _, predictions = logits.max(1)\n",
        "#         num_corr = (predictions == targets).sum()\n",
        "#         running_acc = float(num_corr)/float(data.shape[0])\n",
        "        \n",
        "#         writer.add_scalar(\"Training Loss\", loss, global_step = step) \n",
        "#         writer.add_scalar(\"Accuracy\", running_acc, global_step=step)\n",
        "#         step += 1  \n",
        "\n",
        "\n",
        "#     total_loss.append(train_loss)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize Model NN\n",
        "\n",
        "# model_1 = NNet(hidden_layer_size=100,no_hidden_layers=50).to(device=device)\n",
        "\n",
        "\n",
        "\n",
        "# # Training\n",
        "# # Loss and Optimizer\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model_1.parameters(), lr=lr, momentum= mm)\n",
        "\n",
        "# writer = SummaryWriter(f'runs/MNIST/NN_classic_1024')\n",
        "\n",
        "# total_loss = []\n",
        "\n",
        "# step = 0\n",
        "\n",
        "# for epoch in range(epoch_no):\n",
        "#     train_loss = 0\n",
        "#     for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "#         data = data.to(device=device)\n",
        "#         targets = targets.to(device=device)\n",
        "\n",
        "#         data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "\n",
        "#         # forwards\n",
        "#         logits = model_1(data)\n",
        "#         loss = criterion(logits, targets)\n",
        "#         train_loss += loss.item()\n",
        "\n",
        "\n",
        "#         # backward \n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Running Accuracy\n",
        "\n",
        "#         _, predictions = logits.max(1)\n",
        "#         num_corr = (predictions == targets).sum()\n",
        "#         running_acc = float(num_corr)/float(data.shape[0])\n",
        "        \n",
        "#         writer.add_scalar(\"Training Loss\", loss, global_step = step) \n",
        "#         writer.add_scalar(\"Accuracy\", running_acc, global_step=step)\n",
        "#         step += 1  \n",
        "\n",
        "\n",
        "#     total_loss.append(train_loss)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter: \n",
        "\n",
        "inputs = 784\n",
        "# lr = 1e-2\n",
        "mm = 0\n",
        "batch_sz=64\n",
        "epoch_no = 3\n",
        "mini_batches = [64]\n",
        "learning_rates = [0.1,0.01,0.001,0.0001]\n",
        "learning_rates = learning_rates[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Option 1:\n",
        "\n",
        "def unbiased_data(batch_sz):\n",
        "    train_dataset = MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "    test_dataset = MNIST(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_sz, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_sz, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Option 2:\n",
        "# MNIST\n",
        "def mnist(batch_sz, valid_size=0.2, shuffle=True, random_seed=2000):\n",
        "    num_classes = 10\n",
        "    transform_train = transforms.Compose([\n",
        "                        transforms.RandomCrop(28, padding=4),\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "    \n",
        "    \n",
        "    transform_test = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "    \n",
        "\n",
        "    # Training dataset\n",
        "    train_data = MNIST(root='./datasets', train=True, download=True, transform=transform_train)\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    if shuffle == True:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_sz ,pin_memory=True)\n",
        "\n",
        "    # Test dataset\n",
        "    test_data = MNIST(root='./datasets', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                              batch_size=batch_sz, shuffle=False, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and Testing\n",
        "def var_training(data_bias=True,model=CNN(),type_CNN = True):\n",
        "    for mini_batch in mini_batches:\n",
        "        for learning_rate in learning_rates:\n",
        "            model_2 = model.to(device=device)\n",
        "        \n",
        "            # write to tensorboard\n",
        "            step = 0\n",
        "\n",
        "            if data_bias == True and type_CNN == True:\n",
        "                writer = SummaryWriter(f'runs/MNIST/CNN_rand_bs={mini_batch}_lr={learning_rate}')\n",
        "            elif data_bias == False and type_CNN == True:\n",
        "                writer = SummaryWriter(f'runs/MNIST/CNN_bias_bs={mini_batch}_lr={learning_rate}')\n",
        "            elif data_bias == True and type_CNN == False:\n",
        "                writer = SummaryWriter(f'runs/MNIST/NN_wS_rand_bs={mini_batch}_lr={learning_rate}')\n",
        "            else:\n",
        "                writer = SummaryWriter(f'runs/MNIST/NN_wS_bias_bs={mini_batch}_lr={learning_rate}')\n",
        "\n",
        "\n",
        "            # Loss and optimizer\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.SGD(model_2.parameters(), lr=learning_rate, momentum= mm)\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer,gamma=0.9,verbose=False)\n",
        "\n",
        "            # generate new data\n",
        "\n",
        "            if data_bias == True:\n",
        "                train_loader, test_loader=mnist(mini_batch)\n",
        "\n",
        "            else:\n",
        "                train_loader, test_loader=unbiased_data(mini_batch)\n",
        "            \n",
        "            for epoch in range(epoch_no):\n",
        "                train_loss = 0\n",
        "                for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "                    data = data.to(device=device)\n",
        "                    targets = targets.to(device=device)\n",
        "\n",
        "                    if type_CNN == False:\n",
        "                        data = data.reshape(data.shape[0], -1)\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "                    # forwards\n",
        "                    logits = model_2(data)\n",
        "                    loss = criterion(logits, targets)\n",
        "                    train_loss += loss.item()\n",
        "\n",
        "\n",
        "                    # backward \n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    # scheduler.step()\n",
        "\n",
        "                    # Running Accuracy\n",
        "                    _, predictions = logits.max(1)\n",
        "                    num_corr = (predictions == targets).sum()\n",
        "                    running_acc = float(num_corr)/float(data.shape[0])\n",
        "                    \n",
        "                    writer.add_scalar(\"Training Loss\", loss, global_step = step) \n",
        "                    writer.add_scalar(\"Training Accuracy\", running_acc, global_step=step)\n",
        "                    step += 1  \n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "            acc_l = []\n",
        "\n",
        "            accuracy = tm.Accuracy()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                step_2 = 0\n",
        "                for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "                    data = data.to(device=device)\n",
        "                    targets = targets.to(device=device)\n",
        "\n",
        "                    if type_CNN == False:\n",
        "                        data = data.reshape(data.shape[0], -1)\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "                    logits = model_2(data)\n",
        "                    t_loss = criterion(logits, targets)\n",
        "                    train_loss += loss.item()\n",
        "\n",
        "                    yhat = torch.argmax(logits, axis =1)\n",
        "\n",
        "                    acc = accuracy(yhat.to(\"cpu\"),targets.to(\"cpu\"))\n",
        "\n",
        "                    acc_l.append(acc)\n",
        "\n",
        "                    writer.add_scalar(\"Testing Loss\", t_loss, global_step = step_2) \n",
        "                    writer.add_scalar(\"Testing Accuracy\", acc, global_step=step_2)\n",
        "                    step_2 += 1\n",
        "\n",
        "            print(f'the accuracy on the test set for the batch size: {mini_batch} and learning rate: {learning_rate} is {np.mean(acc_l):.2f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.0001 is 0.11\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.001 is 0.13\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.01 is 0.87\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.1 is 0.97\n"
          ]
        }
      ],
      "source": [
        "var_training(data_bias=True,model=CNN(),type_CNN=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Adjusting learning rate of group 0 to 9.0000e-05.\n",
            "Adjusting learning rate of group 0 to 8.1000e-05.\n",
            "Adjusting learning rate of group 0 to 7.2900e-05.\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.0001 is 0.10\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Adjusting learning rate of group 0 to 9.0000e-04.\n",
            "Adjusting learning rate of group 0 to 8.1000e-04.\n",
            "Adjusting learning rate of group 0 to 7.2900e-04.\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.001 is 0.20\n",
            "Adjusting learning rate of group 0 to 1.0000e-02.\n",
            "Adjusting learning rate of group 0 to 9.0000e-03.\n",
            "Adjusting learning rate of group 0 to 8.1000e-03.\n",
            "Adjusting learning rate of group 0 to 7.2900e-03.\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.01 is 0.74\n",
            "Adjusting learning rate of group 0 to 1.0000e-01.\n",
            "Adjusting learning rate of group 0 to 9.0000e-02.\n",
            "Adjusting learning rate of group 0 to 8.1000e-02.\n",
            "Adjusting learning rate of group 0 to 7.2900e-02.\n",
            "the accuracy on the test set for the batch size: 64 and learning rate: 0.1 is 0.95\n"
          ]
        }
      ],
      "source": [
        "var_training(data_bias=True,model=NN(),type_CNN=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW3_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Summer_School",
      "language": "python",
      "name": "summer_school"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "d410fd81c9a0b51d0e53167c40a880776428168a003798fed6e3e9082ee009aa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
